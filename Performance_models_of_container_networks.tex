%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{media}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.


\usepackage[utf8]{inputenc}


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor vir-tua-li-za-tion}

% include page numbers
\pagenumbering{arabic}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Survey on Performance Models of Container Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{David de Andrés Hernández}
\IEEEauthorblockA{Technical University of Munich\\
Email: deandres.hernandez@tum.de}
}

% make the title area
\maketitle

\begin{abstract}
%\boldmath
When \textit{cloudifying} any system, the choice of an appropriate container network solution is critical. The wrong container network choice can turn a working system unviable and eclipse the main benefits of cloud architectures: reliability, scalability, and flexibility. Yet the number of solutions targeting the cloud environment is vast and the angles approaching the challenges are varied. Therefore, understanding and evaluating the benefits and limitations of the available solutions and technologies is a tedious but crucial process. To support this process, this paper surveys nine published benchmarks and three performance models searching for the major affecting factors and estimators. The findings are the following: the CPU prevails as bottleneck for performance in most scenarios; although many performance benchmarks exist, not many target scenarios with large-scale container pairs; performance models for containers with focus on the CPU resources provide the most precise estimators. 
\end{abstract}

%\IEEEpeerreviewmaketitle

\section{Introduction}
% no \IEEEPARstart
Cloud architectures are motivated by both economies of scale and resource optimization and are enabled by virtualization technologies. But virtualization often incurs in performance degradations and a reduction in the system's portability. To mitigate this effect, containers, a form of lightweight virtualization, emerged. Thanks to their alternative means of partitioning resources, the virtualization overhead is drastically reduced, and the deployment process is expedited. These benefits favoured the standardization of containers and that in turn made portability an additional strength of this technology.

It's popularity rising, a new architecture exploiting container's strengths emerged: microservices. This architecture borrows the encapsulation principle of software development and breaks applications into stand-alone containers. In this approach, each container is only responsible for a single function and by doing so, operators can update or replace components without impact to the remaining services. Moreover, applications can be granularly scaled by load-balancing a service into several instances of the same container. If in addition instances are stateless, containers can be re-spawned in-service and without downtime. Nontheless, the price to pay for microservices is the need of automation and orchestration software to overcome the explosion in the number of components. Overall, microservices and cloud principles are aligned.

In the light of the above, network connectivity among containers is paramount for the correct functioning of microservice-based applications.  Yet, there are two important challenges. First, a microservice-populated cloud changes constantly within seconds and containers can be considered ubiquitous. Second, all possible traffic patterns take place: between containers in the same VM, between containers in different VMs, between containers in different hosts, between containers in different data centres, and many more. In this scenario, the host's OS becomes an important component of the networking infrastructure of the cloud. With one remark, it was not conceived for such scale. For this reason, the networking performance of containers must be carefully considered as it can turn a working system unviable when migrated to cloud architectures. The main cause of this degradation is due to overhead processing of packets through the OS's networking stack. Another source of degradation is the processing of headers belonging to overlay networks. Overlay networks, although complex, allow the communication of containers in constant move. But again, the host's OS where not conceived to process headers efficiently at this scale. Willing to overcome the performance challenge, operators have created sophisticated frameworks for high-performance packet IO.

Due to the immense number of choices for container networking, an evaluation of the solutions is of vital importance. To be thorough, this evaluation requires of several steps. The first step is to analyse the solutions to confirm whether the technological requirements of the system to be \textit{cloudified} can be satisfied, or not. Further, consulting benchmarks can give an indication of the bottlenecks and viability. In the last stages, a high-fidelity model can estimate the results that configuration changes can have in the system's behaviour.

\section{Background}
\subsection{Networking stacks}
Most container environments make use of the kernel's network stack because it is feature rich, reliable and easy to use. However in high-performance scenarios, custom-made stacks running in user space can replace the kernel's implementation to provide additional performance at the cost of additional complexity.  
\subsubsection{Kernel's stack}
\begin{figure}[!t]
\centering
\includegraphics[scale=0.6]{kernel_stack.drawio.pdf}
%\includegraphics[width=2.5in]{ }
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Kernel's network stack \cite{kernel_stack} and DPDK stack [self-made].}
\label{kernel_stack}
\end{figure}
Current OS's include a rich network stack providing a socket-based user-level interface for transmitting and receiving packets; handling a wide variety of protocols; as well as managing the underlying hardware. Figure \ref{kernel_stack} presents on the left the different layers which packets traverse before being handed over to a user-space application. At the bottom we find the device driver, which is the layer responsible for interacting directly with the HW. This includes: claiming control of a device; requesting memory ranges and IO ports; setting the DMA mask; and registering functions to send, receive and manipulate packets. Next in the stack, we find the Network Device Driver Interface (NDDI), which enables, multiple and perhaps different, network devices to be used simultaneously. Furthermore, the NDDI includes a packet scheduler implementing queuing disciplines. Moving upwards, the protocol layer is where the different protocols are implemented. Each protocol must interact to the north with the socket interface and to the south with the NDDI. To do this, each protocol is associated with a protocol family (PF\_*) northbound, and with a protocol type southbound. At the top of the kernel stack we find the Berkeley Socket Interface, which allows user space programs to communicate with the remote devices. Is the last abstraction layer which gives programs the impression of communicating directly. At this layer, every socket type is associated with a protocol. For example, the PF\_INET is associated with the TCP/IP protocol. By introducing all these layers, the stack remains very flexible and can accommodate features with reduced effort; nevertheless this flexibility is in part responsible for performance losses.

\subsubsection{User space stacks}
Besides adding complexity by re-implementing the network stack, high-performance IO frameworks are key-enablers for Containerized Network Functions (CNF) \cite{SIGARCH_2017:Yang}. Sometimes, the variety of protocols and functions which the kernel's networking stack offers are not required. This makes it feasible to re-write the required portions of stack using a high-performance IO framework. DPDK \cite{dpdk} and PF\_RING ZC \cite{pf_ring_zc} can lead to a nine-fold performance increase over the default kernel IO framework \cite{ANCS:Gallenmuller}. Figure \ref{kernel_stack} presents on the right the layers of DPDK networking stack for comparison. At the bottom we find the kernel driver, a minimal layer which loads and binds the ports to the poll-mode driver in user space. This is the only component which lies within the kernel space. In the user space we find the poll-mode driver. This is a key component for enabling high performance packet processing. In contrast to the kernel stack, which is interrupt-driven, the DPDK stack polls the NIC continuously. This in turns, consumes all available CPU cycles; in case no packets are available for processing, the CPU cycles are wasted (busy waiting). At the top we find the DPDK libraries, which provide utilities for high-performance packet processing applications. Introducing frameworks such as DPDK in container environments has already been accomplished and is a production ready approach. Examples are: Open vSwitch \cite{ovs-dpdk} and Tungsten Fabric \cite{tungsten-dpdk}. The vRouter from the Tungsten Fabric project, for example, uses DPDK to efficiently route, encapsulate and decapsulate the packets.
\subsection{Container Networks}
We have seen the importance of container communication in microservices architectures. To materialize this communication, there are multiple options, each with its own drawbacks and advantages. In the following we differentiate between modes for inter-host communication, and services for intra-host communication. This classifications aims to support the evaluation process. Please note that these modes are not exclusive and usually they co-exist.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{container_network.drawio.pdf}
\caption{Available kernel device drivers for intra-host communications.}
\label{virtual_networks}
\end{figure*}
\subsubsection{Modes for intra-host communication}\hfill\break
\textbf{None Mode}\hspace{0.2cm}In this mode, a container is isolated into its own namespace. A namespace is a logical copy of the OS’s network stack with only a loopback interface. Because of this, it cannot communicate with other containers. Achieving extreme isolation, it is suitable for offline computation such as batch processing or backup jobs.

\noindent\textbf{Bridge Mode}\hspace{0.2cm} In this mode, the key component is a virtual bridge created inside a specified namespace. In addition, this mode uses linux's veth device drivers. Veth pairs serve as pipes between namespaces, as depicted in figure \ref{virtual_networks}. Containers can then be launched including a pair of veth interfaces, where one of the pairs will be moved to the namespace of the bridge (and enslaved with it) and the other pair will remain in the container's namespace. Furthermore, the bridge can be enhanced with L3 communication by giving each veth interface an IP address within the bridge's network subnet. This mode allows star-like topologies but does not bring alone connectivity to external networks. For external connectivity other services, such as NAT or overlays must be configured. 

\noindent\textbf{Container Mode}\hspace{0.2cm} The container mode can be seen as an extension of the None mode. First, a container is spawned in None mode, thus creating a dedicated networking namespace. Subsequent containers are launched inside this namespace by providing the namespace's id as a launch parameter. What this effectively does is sharing a single namespace across containers. As a consequence, all containers share the access to the interfaces within this namespace as well as firewall rules and ip routes. The level of isolation is reduced but containers benefit from standard inter-process communication (IPC) and hence, suffer less overhead. This mode is often seen in Kubernetes environments under the name of pods. Pods are a group of containers that work together.

\noindent\textbf{Host mode}\hspace{0.2cm}In this mode, containers share the host's OS networking stack. Consequently, all containers can communicate with each other through IPC. Additionally, the host can provide external connectivity while sharing the same IP addresses and port ranges. This is the lowest level of security and flexibility. Effectively, host mode is often used as performance baseline because its networking overhead is the smallest.

\noindent\textbf{Macvlan \& Ipvlan}\hspace{0.2cm} Like VLAN tags, which allow to logically partition a HW interface, these drivers allow the creation of multiple L2/L3 interfaces with individual addressing on top of a single HW interface. By doing this, a MAC or IP address, depending on the use case, can be assigned to a container, making it appear as a physical device on the network. To implement this, both the Macvlan and Ipvlan kernel modules get enslaved with the driver of the NIC (master) in kernel space, as shown in figure \ref{virtual_networks}. The enslaved interfaces now share the same broadcast domain, although whether communication between the interfaces is allowed depends on the configured Macvlan and Ipvlan type. The main difference between both is whether the processing of the packets will happen up to L2 (Macvlan) or up to L3 (Ipvlan) in the slave namespace stack. The respective configuration modes of each module determine what type of communication will be allowed between slave interfaces.
\subsubsection{Network Services for inter-host communication}\hfill\break
\textbf{Network Address Translation}\hspace{0.2cm} This service can be used on top of bridged mode to provide external connectivity. This approach adds a rule to the NAT table for each container. The rule then maps a port number to a container private IP. When a container sends a packet, the bridge remaps the source (private) IP to the host's (public) IP. Upon reception of packets, the host checks the destination port and the NAT table and performs the corresponding address translation. Although this approach is simple, it incurs a significant processing overhead which leads to a performance loss. In addition, because of the port range constraint, port-conflicts can arise in environments with short-lived containers. Nonetheless, NAT also provides some benefits. Thanks to the address translation, the container's network inside the host is decoupled from the external network. In other words, instead of having to allocate public addresses for each container, a single public IP is required and changes in the external network do not influence the hosts networks.

\noindent\textbf{Overlay Networks}\hspace{0.2cm}An overlay network is a further level of abstraction. In this scenario an underlay network ensures network reachability between hosts. While the overlay, which is tunnelled through the underlay, provides connectivity between containers. Essentially, containers have the impression that they are directly connected to other containers. This abstraction allows great flexibility when deploying containers which need to communicate but cannot be launched within the same host, for instance because of resource limitations. Additionally, an overlay should be resilient to changes in the underlay, thus providing additional flexibility. The choice of the tunnelling protocol depends on whether L2 (VXLAN) or L3 (IPIP, MPLSoGRE, MPLSoUDP, etc.) connectivity is required and what the underlying infrastructure supports. To be able to create the tunnels, containers must share a mapping between their private address and their host's public address. This is usually done in the form of a key-value (KV) store available to all nodes. Overlays are usually based on the kernel's TUN/TAP device driver \cite{tuntap}. In essence, this device driver links the kernel's network stack and a program in user space. Instead of receiving packets from a physical interface, it receives them from a user space program and vice-versa. This is depicted in figure \ref{virtual_networks}. While TUN devices are used with programs that read/write IP packets, TAP devices are used when programs handle Ethernet frames. For overlay networking, a TAP device is used to send the frames leaving a container to the program responsible for the encapsulation and decapsulation.

\noindent\textbf{Routing}\hspace{0.2cm} Another option to provide inter-host communication is to use routing. If a software router is deployed within each host then routing protocols such as BGP can be used to exchange reachability information of containers running on different hosts and on different networks. Thanks to the flexibility of BGP, different virtual routing functions (VRFs) can be created to allow overlapping IP ranges between hosts. This solution is limited to the protocols supported by the software router as well as the routing table scale.
\section{Benchmarks}

\begin{table*}[!h]

 \begin{center}

   \caption{ Classification of Studies analysing the performance of standard container networking frameworks}\label{tab:1}

   \begin{tabular}{c c c c c c c c c}
     \hline
     % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Study & Year & Inter- or & K8s-CNI/& Network mode & Comm. pairs & Measurement& Comments  \\ 
      &  & Intra-host & Docker-plugins & & & tools &   \\ 
     \hline
     \cite{IEEE_INFOCOM_2018:K. Suo} & 2018 & both & Calico, Weave, Flannel, & Host, NAT, VXLAN, BGP & 1 & Netperf, Sockperf, Sparkyfish,& & \\
     & & & libnetwork & & & OSU Benchmark & & \\     
     \cite{HotConNet_17:Zhao} & 2017 & both & libnetwork & Veth, Macvlan & 1 & iperf3, pktgen & & \\
     \cite{Boeira:2021} & 2021 & intra & libnetwork & Bridge, Macvlan, OvS & 50 & iperf, netcat, sockperf & &\\
     \cite{Bankston:2018} & 2018 & inter & Calico, Weave, Flannel, & VXLAN, BGP & 1 & iperf3 & Public clouds: AWS,\\
      & & & libnetwork & & & & Azure, GCP \\
     \cite{ICTC_2018:Park} & 2018 & both & Flannel & VXLAN, OvS & 1 & iperf & Openstack with Kuryr\\
     \cite{NOMS_2016:Claassen} & 2016 & both & libnetwork & Veth, Ipvlan, Macvlan, OvS & 1-128 & iperf3 & &\\
     \hline 

   \end{tabular}

 \end{center}
\end{table*} 

\begin{table*}[!h]

 \begin{center}

   \caption{ Classification of Studies analysing the performance of special container networking frameworks}\label{tab:2}

   \begin{tabular}{c c c c c c c c c}
     \hline
     % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Study & Year & Inter- or & K8s-CNI/& Network mode & Comm. pairs & Measurement& Comments  \\ 
      &  & Intra-host & Docker-plugins & & & tools &   \\ 
     \hline
     \cite{Nakamura:2018} & 2018 & intra & Custom proto. & AF\_Graft & 50 & iperf3, sockperf3 & \\     
     \cite{HotNets:16:FreeFlow} & 2016 & both & Weave and Custom proto. & MPI/DPDK vs. VXLAN &1 & iperf & & \\
     \cite{CoNEXT:2018} & 2018 & inter & Cilium & eBPF/XDP vs. DPDK & 1 & trex & Artifacts evaluated and reusable & \\
     \hline 

   \end{tabular}

 \end{center}
\end{table*} 

In this section we present and classify a compendium of performance benchmarks for container networks available in the literature: 
\cite{IEEE_INFOCOM_2018:K. Suo}, \cite{HotConNet_17:Zhao}, \cite{Nakamura:2018}, \cite{Boeira:2021}, \cite{Bankston:2018}, \cite{CoNEXT:2018}, \cite{ICTC_2018:Park}, \cite{NOMS_2016:Claassen}. Thanks to this classification it is possible to identify the scenarios which have already been studied and which not.\\
First, we summarize in table \ref{tab:1} the characteristics of the studies limited to standard frameworks. We observe that most studies focus on a single pair of communicating container pairs. The exceptions are [8] and [13] which include inter-host communicating pairs --- an expected traffic pattern in microservice architectures. Further analysis shows that most evaluations include libnetwork (docker's underlying networking library) because of its high adoption. However, with the deprecation of Docker as container runtime for Kubernetes, and the adoption of the CNI model, instead of CNM/libnetwork, these results might soon be of little interest. In relation to the network modes, the typical scenarios are all covered; particularly Macvlan and OvS are the most studied. Moreover, the measurement tools focus on L3/L4 protocols because of their prevalence; however CNF's would also benefit from L2 performance indicators. Finally, it is worth mentioning that \cite{Bankston:2018} focuses on public clouds which are not otherwise evaluated.\\
Second, we compare in table \ref{tab:2} the studies which include alternative means for achieving higher performance. It stands out, that to achieve higher performance all three frameworks move away from the OS's default network path and use either DPDK, XDP/eBPF or a custom driver. This move effectively reduces the processing overhead; nevertheless, the CPU remains to be the bottleneck. Worth mentioning is the effort put by \cite{CoNEXT:2018} to make their artefacts available to the community and thus, allowing further researchers to build on top.
\section{Factors Analysis}
In this section we break down the different factors that influence the performance of the presented container networks. This allows us to better analyse and reuse benchmark results.

\noindent\textbf{Packet size}\hspace{0.2cm} 
%\subsection{Packet size}
Note that it is only slightly more expensive to send a 1.5KB packet than sending a 64B packet \cite{Rizzo:2012}. For this reason, it is useful to to provide the packet rate in packets per second, pps, and indicate additionally the packet size. The average packet size that a container needs to send, receive or manipulate has consequently a big impact on the throughput. Smaller packet sizes imply more packets, what in turn means more CPU cycles being consumed.

\noindent\textbf{Transport Protocol}\hspace{0.2cm} 
%\subsection{Transport Protocol}
The linux kernel includes optimizations for TCP, such as GRO \cite{GRO}, that allows TCP to perform better than UDP \cite{HotConNet_17:Zhao}. This is a possible explanation to the important performance degradation observed in \cite{NOMS_2016:Claasen}. In this study, the authors observe that the UDP throughput is 3.5 times lower than the TCP throughput when using the ipvlan and macvlan drivers.

\noindent\textbf{Latency Budget}\hspace{0.2cm}
To reduce the number of interruptions and sustain high throughput rates when the incoming rate is high, packets are buffered before being sent to the network interface card \cite{DEBS_20:Stylianopoulos}. \cite{ANCS:Gallenmuller} points out that larger buffer sizes not only increase the throughput but also the average latency. As a result, if the latency budget is limited by the application requirements, the achievable throughput must be reduced by means of reducing the buffer size.

\noindent\textbf{Virtualization layers}\hspace{0.2cm}
The number of virtualization layers causes significant overheads. When running containers inside VMs, the packet's data is copied from the hypervisor's kernel space to the user space, where the VM resides. Once in the VM, the virtualized kernel must again copy the data to the VM’s user space. This additional copy actions as well as context swapping results in performance degradation. In other words, adding virtualization layers increases the packet's critical path. \cite{IEEE_INFOCOM_2018:K. Suo} reports a 42\% loss in the TCP throughput when running the containers inside a VM.

\noindent\textbf{Containers-Resources Ratio}\hspace{0.2cm}
Another factor is the number of containers within one host which must compete for resources. Most of the container network options make a noticeable use of CPU resources for either NAT or overlay services. Therefore, an elevated number of containers competing for CPU resources interfere each other with multiple context changes. Furthermore, if the CPU becomes the bottleneck, then packets need to be queued incurring additional delays. To remedy this, CPUs can be pinned to specific containers so that they become exclusive. \cite{Boeira:2021} shows this effect, namely that the average throughput decreases and the flow completion times increase with an increasing containers-to-core ratio. 

\noindent\textbf{Network driver}\hspace{0.2cm}
The implementation of the different network drivers and services has also a relevant performance impact. \cite{IEEE_INFOCOM_2018:K. Suo} shows that containers in the same host using bridge mode incur 18\% and 30\% throughput loss in upload and download, respectively in comparison to the baseline (host). In \cite{NOMS_2016:Claasen}, ipvlan and macvlan perform up to 3 times better than veth bridges. For this reason, linux bridges should be avoided when possible in favour of macvlan or ipvlan.

\noindent\textbf{Network Services}\hspace{0.2cm}
While improving flexibility, the encapsulation and decapsulation operations increase the critical path, and thus consume additional clock cycles. Furthermore, the additional headers reduce the payload size (the MTU of the underlay must be respected). Last, possessing the key-value mapping is a requisite for establishing the tunnel. Consequently, the time required for their distribution limits the container's start.

\noindent\textbf{Encryption}\hspace{0.2cm}
In cloud environments with multiple tenants, packet encryption is recommended as an additional security layer. The lower the encryption is performed, the higher security degree is achieved. The cryptographic operations have a relevant performance impact which affects both the packet rate as well as the packet delay. The impact of using IPSec has been captured in \cite{IPsec:2002:Miltchev}, \cite{Ferrante:2005}, and \cite{ICCET:2010} for LINUX systems. These studies show a 25-33\% performance degradation. Still, similar studies in container environments are missing to confirm the applicability of these results. 

\section{Models}
Having a precise mathematical model whose parameters can be manipulated to observe possible reactions is a very powerful tool. However, modelling complex large-scale systems as a containers network is a complex task. The number of abstraction layers, concurrent processes and middleware makes modelling with high precision an intractable task. This does not mean that it shouldn't be done at all. With the appropriate simplifications and assumptions, a model can provide an approximate result saving a lot of simulation and/or experimentation time.

Gallenmuller et al. \cite{ANCS:Gallenmuller} survey various frameworks for high-performance packet IO and introduce a model to estimate and assess their performance. The model builds on top of \cite{Rizzo:2012} which claims that packet processing costs can be divided into per-byte and per-packet cost; for IO frameworks, per-packet costs dominate. Two assumptions follow: (1) per-packet costs are constant for high performance IO frameworks, (2) measurements are performed under the most demanding circumstances if the highest packet rate is chosen, i.e. 64B packets. Further analysis leads to
\begin{align}
f^{CPU} = n \cdot (c_{IO}+c_{task}+c_{busy})
\end{align}
\noindent where $n$ represents the pps; $f^{CPU}$ describes the available CPU cycles; $c_{IO}$ represents the framework's costs for sending and receiving packets, which are constant by the first assumption; $c_{task}$ are the costs of the application running on top of the framework, and depend of the complexity of the processing task; and $c_{busy}$ which are the costs introduced by the busy wait i.e. polling the NIC. Recall that, in the case of container networks, overlay encapsulation or NAT would be included in $c_{IO}$, while the containers application logic is represented in $c_{task}$.
The presented measurements prove the accuracy of the proposed model. Consequently, this model can be used to assess the number of containers which can run concurrently within a single host. This removes the possible performance degradation due to interference.

Medel et al. \cite{UCC_2016:Medel} present a performance and resource management model of Kubernetes based on Object Nets \cite{ObjectNets} (a type of Petri Net \cite{PetriNets}). Petri Nets, also known as a place/transition nets, are a formal modelling tool used to describe the behaviour of concurrent and distributed systems. A place represents the state of the system, while transitions represent the actions that can trigger a state change. Places and Transitions are connected by means of arcs. Further, places in a Petri Net may contain a discrete number of marks called tokens. The tokens represent resources which are available for the firing of a transition. Such transition may only be enabled, i.e. ready to be fired, if the required number of tokens are available in the Place. Medel et al. characterize their proposed model using real data from a Kubernetes deployment and suggest that it can be used to design scalable applications. Furthermore, Medel et al. consider the characterization of several overhead sources, including networking. They consider two networking scenarios: (i) one pod is deployed and all containers are inside that pod; and (ii) several pods are deployed with exactly one container each. Their results suggest that deploying several pods with a few coupled containers is better than a single pod with a large number of containers. This allows them to characterize the overhead of the placement of the containers. Further study for the characterization of the intra-host communication overhead is unfortunately not provided.

Khazaei et al. \cite{IEEE_2012:Khazaei} describe an approximate analytical model for performance evaluation of cloud server farms and solve it to obtain accurate estimation of the complete probability distribution of the request response time and other important performance indicators. This analytical model, although conceived to represent dependencies between host in data-centres, could be adapted to characterize container environments due to the similarities between both cases. In the original work, Khazaei et al., propose a M/G/m/m+r queuing system to model the data-center. For the performance analysis they combine a transform-based analytical model and an approximate Markov chain model. This approach allows them to obtain a complete probability distribution of the performance indicators. Khazaei et al. make a remark which might be extrapolable to the container environment:  the kurtosis values for two of their experiments show that in heterogeneous cloud centres (i.e., those for which the coefficient of variation of task service time is higher) it is more difficult to maintain Service Level Agreements (SLA) compared to homogeneous centres. In the container world we could expect that in hosts running heterogeneous container networks, meaning a variety of drivers and network services, it will be more difficult to ensure specific performance levels. Consequently, it might be sensible to group containers by driver type and services. Further work is required to validate these statements.
\section{Conclusion}
In this paper I have analysed the characteristics of available networking stacks as well as of available container networks. This analysis, paired up with the application requirements shall allow the reader to select the most suitable container network solution. Furthermore, we have seen that many performance benchmarks exist. With consideration of the factors affecting the performance, these benchmarks are a good indication of the performance that can be expected. Yet, the customizability of container environments and the fast pace, at which container technologies evolve, reduce the chances of re-using, or even finding, applicable benchmark results. For this reason, parametrical models would be of high value to the field. Indeed, we have seen in this paper, that the modelling of the CPU usage --- the current performance bottleneck--- can provide valuable insights of the performance of container networks.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\newpage
\begin{thebibliography}{1}

\bibitem{IEEE_INFOCOM_2018:K. Suo}
K. Suo, Y. Zhao, W. Chen and J. Rao, "An Analysis and Empirical Study of Container Networks," IEEE INFOCOM 2018 - IEEE Conference on Computer Communications, 2018, pp. 189-197, doi: 10.1109/INFOCOM.2018.8485865.

\bibitem{ANCS:Gallenmuller}
S. Gallenmuller, P. Emmerich, F. Wohlfart, D. Raumer and G. Carle, "Comparison of frameworks for high-performance packet IO," 2015 ACM/IEEE Symposium on Architectures for Networking and Communications Systems (ANCS), 2015, pp. 29-38, doi: 10.1109/ANCS.2015.7110118.

\bibitem{HotConNet_17:Zhao}
Yang Zhao, Nai Xia, Chen Tian, Bo Li, Yizhou Tang, Yi Wang, Gong Zhang, Rui Li, and Alex X. Liu. 2017. Performance of Container Networking Technologies. In Proceedings of the Workshop on Hot Topics in Container Networking and Networked Systems (HotConNet '17). Association for Computing Machinery, New York, NY, USA, 1–6. https://doi.org/10.1145/3094405.3094406

\bibitem{DEBS_20:Stylianopoulos}
Charalampos Stylianopoulos, Magnus Almgren, Olaf Landsiedel, Marina Papatriantafilou, Trevor Neish, Linus Gillander, Bengt Johansson, and Staffan Bonnier. 2020. On the performance of commodity hardware for low latency and low jitter packet processing. In Proceedings of the 14th ACM International Conference on Distributed and Event-based Systems (DEBS '20). Association for Computing Machinery, New York, NY, USA, 177–182. https://doi.org/10.1145/3401025.3403591

\bibitem{NOMS_2016:Claasen}
J. Claassen, R. Koning and P. Grosso, "Linux containers networking: Performance and scalability of kernel modules," NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium, 2016, pp. 713-717, doi: 10.1109/NOMS.2016.7502883.

\bibitem{Rizzo:2012}
Luigi Rizzo. 2012. Netmap: a novel framework for fast packet I/O. In Proceedings of the 2012 USENIX conference on Annual Technical Conference (USENIX ATC'12). USENIX Association, USA, 9.

\bibitem{Nakamura:2018}
Ryo Nakamura, Yuji Sekiya, and Hajime Tazaki. 2018. Grafting sockets for fast container networking. In Proceedings of the 2018 Symposium on Architectures for Networking and Communications Systems (ANCS '18). Association for Computing Machinery, New York, NY, USA, 15–27. https://doi.org/10.1145/3230718.3230723

\bibitem{HotNets:16:FreeFlow}
Tianlong Yu, Shadi Abdollahian Noghabi, Shachar Raindel, Hongqiang Liu, Jitu Padhye, and Vyas Sekar. 2016. FreeFlow: High Performance Container Networking. In Proceedings of the 15th ACM Workshop on Hot Topics in Networks (HotNets '16). Association for Computing Machinery, New York, NY, USA, 43–49. https://doi.org/10.1145/3005745.3005756

\bibitem{Boeira:2021}
C. Boeira, M. Neves, T. Ferreto and I. Haque, "Characterizing network performance of single-node large-scale container deployments," 2021 IEEE 10th International Conference on Cloud Networking (CloudNet), 2021, pp. 97-103, doi: 10.1109/CloudNet53349.2021.9657138.

\bibitem{Bankston:2018}
R. Bankston and J. Guo, "Performance of Container Network Technologies in Cloud Environments," 2018 IEEE International Conference on Electro/Information Technology (EIT), 2018, pp. 0277-0283, doi: 10.1109/EIT.2018.8500285.

\bibitem{CoNEXT:2018}
Toke Høiland-Jørgensen, Jesper Dangaard Brouer, Daniel Borkmann, John Fastabend, Tom Herbert, David Ahern, and David Miller. 2018. The eXpress data path: fast programmable packet processing in the operating system kernel. In Proceedings of the 14th International Conference on emerging Networking EXperiments and Technologies (CoNEXT '18). Association for Computing Machinery, New York, NY, USA, 54–66. https://doi.org/10.1145/3281411.3281443

\bibitem{ICTC_2018:Park}
Y. Park, H. Yang and Y. Kim, "Performance Analysis of CNI (Container Networking Interface) based Container Network," 2018 International Conference on Information and Communication Technology Convergence (ICTC), 2018, pp. 248-250, doi: 10.1109/ICTC.2018.8539382.

\bibitem{NOMS_2016:Claassen}
J. Claassen, R. Koning and P. Grosso, "Linux containers networking: Performance and scalability of kernel modules," NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium, 2016, pp. 713-717, doi: 10.1109/NOMS.2016.7502883.

\bibitem{SIGARCH_2017:Yang}
Yang Hu, Mingcong Song, and Tao Li. 2017. Towards "Full Containerization" in Containerized Network Function Virtualization. SIGARCH Comput. Archit. News 45, 1 (March 2017), 467–481. https://doi.org/10.1145/3093337.3037713

\bibitem{UCC_2016:Medel}
V. Medel, O. Rana, J. Á. Bañares and U. Arronategui, "Modelling Performance \& Resource Management in Kubernetes," 2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC), 2016, pp. 257-262.

\bibitem{IEEE_2012:Khazaei}
H. Khazaei, J. Misic and V. B. Misic, "Performance Analysis of Cloud Computing Centers Using M/G/m/m+r Queuing Systems," in IEEE Transactions on Parallel and Distributed Systems, vol. 23, no. 5, pp. 936-943, May 2012, doi: 10.1109/TPDS.2011.199.

\bibitem{ObjectNets}
R. Valk, "Object petri nets: Using the nets-within-nets paradigm advanced course on petri nets 2003" in , pp. 3098, 2003.

\bibitem{PetriNets}
T. Murata, "Petri nets: Properties analysis and applications", Proceedings of the IEEE, vol. 77, no. 4, pp. 541-580, 1989.

\bibitem{IPsec:2002:Miltchev}
Miltchev, S., Ioannidis, S. and Keromytis, A. (2002) A Study of the Relative Costs of Network Security Protocols. Computer Science at Columbia University. Available at: http://www.cs.columbia.edu/~angelos/Papers/ipsecspeed.pdf

\bibitem{Ferrante:2005}
A. Ferrante, V. Piuri and J. Owen, "IPSec hardware resource requirements evaluation," Next Generation Internet Networks, 2005, 2005, pp. 240-246, doi: 10.1109/NGI.2005.1431672.

\bibitem{ICCET:2010}
N. Kazemi, A. L. Wijesinha and R. Karne, "Evaluation of IPsec overhead for VoIP using a bare PC," 2010 2nd International Conference on Computer Engineering and Technology, 2010, pp. V2-586-V2-589, doi: 10.1109/ICCET.2010.5485628.

\bibitem{ipvlan}
\url{https://www.kernel.org/doc/html/v5.12/_sources/networking/ipvlan.rst.txt}

\bibitem{tuntap}
\url{https://www.kernel.org/doc/Documentation/networking/tuntap.txt}

\bibitem{dpdk}
"Data Plane Development Kit: Programmer's Guide, Revision 22.07.0-rc1" Linux Foundation, 2022.

\bibitem{ovs-dpdk}
\url{https://www.intel.com/content/www/us/en/developer/articles/technical/using-docker-containers-with-open-vswitch-and-dpdk-on-ubuntu-1710.html}

\bibitem{tungsten-dpdk}
\url{https://www.dpdk.org/wp-content/uploads/sites/35/2018/12/ZhaoyanYipeng_Tungsten-Fabric-Optimization-by-DPDK.pdf}

\bibitem{pf_ring_zc}
\url{https://www.ntop.org/products/packet-capture/pf_ring/pf_ring-zc-zero-copy/}

\bibitem{kernel_stack}
\url{http://affix.sourceforge.net/affix-doc/c190.html}

\bibitem{GRO}
H. Xu, “Generic receive offload,” in Japan Linux Symposium, 2009

\end{thebibliography}

% that's all folks
\end{document}


